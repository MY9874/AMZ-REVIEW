{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1. Dataset generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/ns3xhsyx7gxdvc917t999gxh0000gn/T/ipykernel_56981/2640375621.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data = pd.read_csv('/Users/libin/Desktop/usc/cs544/HW3/amazon_reviews_us_Beauty_v1_00.tsv', delimiter = '\\t', usecols = ['star_rating', 'review_body'], on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('data.tsv', delimiter = '\\t', usecols = ['star_rating', 'review_body'], on_bad_lines='skip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "outputs": [],
   "source": [
    "raw_data['review_body'] = raw_data.review_body.astype(str)\n",
    "# drop NA from raw data\n",
    "raw_data_1 = raw_data.dropna(axis = 0, how = 'any')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generate balanced dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "outputs": [],
   "source": [
    "raw_data_2 = raw_data_1.copy()\n",
    "raw_data_2['star_rating'] = raw_data_2.star_rating.astype(str)\n",
    "# classify and sample from each class\n",
    "data_dict = {}\n",
    "data_dict[1] = raw_data_2.loc[(raw_data_2['star_rating'] == '1') | (raw_data_2['star_rating'] == '2')].sample(n = 20000, replace = False)\n",
    "data_dict[2] = raw_data_2.loc[raw_data_2['star_rating'] == '3'].sample(n = 20000, replace = False)\n",
    "data_dict[3] = raw_data_2.loc[(raw_data_2['star_rating'] == '4') | (raw_data_2['star_rating'] == '5')].sample(n = 20000, replace = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "outputs": [],
   "source": [
    "ordered_data = pd.concat([data_dict[1], data_dict[2], data_dict[3]], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## do some preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "outputs": [],
   "source": [
    "data_3 = ordered_data.copy()\n",
    "# use only review_body\n",
    "data_3['review'] = data_3['review_body']\n",
    "# remove extra whitespace\n",
    "data_3['review'] = data_3['review'].replace(r\"\\s+\", ' ', regex = True)\n",
    "data_3 = data_3.drop(columns = ['review_body'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [],
   "source": [
    "# convert all characters to lower case\n",
    "data_3['review'] = data_3['review'].str.lower()\n",
    "# remove url\n",
    "data_3['review'] = data_3['review'].replace(r\"<.*?>\", \" \", regex=True)\n",
    "# remove http\n",
    "data_3['review'] = data_3['review'].replace(r\"http*\\S+|www*\\S+\", \" \", regex=True)\n",
    "# remove non-alphabetical characters\n",
    "data_3['review'] = data_3['review'].replace(r\"[^0-9a-zA-Z]\", \" \", regex=True)\n",
    "# remove extra whitespaces\n",
    "data_3['review'] = data_3['review'].replace(r\"\\s+\", \" \", regex=True)\n",
    "data_3 = data_3.dropna(axis = 0, how = 'any')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "outputs": [],
   "source": [
    "# do contractions\n",
    "import contractions\n",
    "def do_contractions(text: str):\n",
    "    result = []\n",
    "    for word in text.split():\n",
    "        result.append(contractions.fix(word))\n",
    "    return \" \".join(result)\n",
    "data_4 = data_3.copy()\n",
    "\n",
    "# do some customized contractions\n",
    "data_4['review'] = data_4['review'].apply(do_contractions)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'dont', 'do not', regex=True)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'wont', 'will not', regex=True)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'cant', 'can not', regex=True)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'wouldnt', 'would not', regex=True)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'didnt', 'did not', regex=True)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'doesnt', 'does not', regex=True)\n",
    "\n",
    "data_4['review'] = data_4['review'].replace(r'll', 'will', regex=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## label the three classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "outputs": [],
   "source": [
    "def classify(rating):\n",
    "    if rating == 1 or rating == 2:\n",
    "        return 1\n",
    "    elif rating == 3:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "data_4.star_rating = data_4.star_rating.astype(int)\n",
    "data_4['star_rating'] = data_4['star_rating'].apply(classify)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "outputs": [],
   "source": [
    "import sklearn\n",
    "data_4 = sklearn.utils.shuffle(data_4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## divide into training and testing set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data_4, test_size=0.2, train_size=0.8, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2. Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (a) Load the pretrained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For google-news model, the similarity between 'buy' and 'purchase' is 0.76\n",
      "For google-news model, the similarity between 'good' and 'nice' is 0.68\n",
      "For google-news model, the similarity between 'king - man + woman' and 'queen' is  0.73005176\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "pairs = [\n",
    "    ('buy', 'purchase'),\n",
    "    ('good', 'nice'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('For google-news model, the similarity between %r and %r is %.2f' % (w1, w2, wv.similarity(w1, w2)))\n",
    "\n",
    "theVec = wv['king'] - wv['man'] + wv['woman']\n",
    "cos_sim = dot(theVec, wv['queen'])/(norm(theVec)*norm(wv['queen']))\n",
    "print('For google-news model, the similarity between \\'king - man + woman\\' and \\'queen\\' is ', cos_sim)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (b) Train my own model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "outputs": [],
   "source": [
    "# output my data to a .txt file, for further training model\n",
    "# Note: Please modify the path to generate and read the file!\n",
    "thePath = '/Users/libin/Desktop/usc/cs544/HW3/'\n",
    "output_data = data_4['review']\n",
    "fullPath = thePath + 'test.txt'\n",
    "output_data.to_csv(fullPath, index=False, header=False, sep='\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath(fullPath)\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "myWord2Vec = gensim.models.Word2Vec(sentences=sentences, vector_size=300, window=13, min_count=9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my own model, the similarity between 'buy' and 'purchase' is 0.80\n",
      "For my own model, the similarity between 'good' and 'nice' is 0.60\n",
      "For my own model, the similarity between 'king - man + woman' and 'queen' is  0.25065425\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('buy', 'purchase'),\n",
    "    ('good', 'nice'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('For my own model, the similarity between %r and %r is %.2f' % (w1, w2, myWord2Vec.wv.similarity(w1, w2)))\n",
    "\n",
    "theVec = myWord2Vec.wv['king'] - myWord2Vec.wv['man'] + myWord2Vec.wv['woman']\n",
    "cos_sim = dot(theVec, myWord2Vec.wv['queen'])/(norm(theVec)*norm(myWord2Vec.wv['queen']))\n",
    "print('For my own model, the similarity between \\'king - man + woman\\' and \\'queen\\' is ', cos_sim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3. Simple Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (a) Prepare feature vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/libin/opt/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/libin/opt/miniconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "# compute the mean word2vec feature of each review as input\n",
    "def getFeature(text):\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            vectors.append(wv[word])\n",
    "    vector = np.mean(vectors, axis=0)\n",
    "    return vector\n",
    "\n",
    "train = train_data.copy()\n",
    "test = test_data.copy()\n",
    "train['input'] = train['review'].apply(getFeature)\n",
    "test['input'] = test['review'].apply(getFeature)\n",
    "train = train.drop(columns='review')\n",
    "test = test.drop(columns='review')\n",
    "train = train.dropna(axis = 0, how = 'any')\n",
    "test = test.dropna(axis = 0, how = 'any')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "outputs": [],
   "source": [
    "X_train = train['input'].to_numpy().tolist()\n",
    "X_test = test['input'].to_numpy().tolist()\n",
    "Y_train = train['star_rating']\n",
    "Y_test = test['star_rating']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (b) Perceprton"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Perceptron is  0.5932415519399249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = Perceptron(max_iter=8000, alpha= 0.1, early_stopping=True)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "perceptron_accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print('The accuracy of Perceptron is ', perceptron_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (c) SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of SVM is  0.6593241551939925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(C=0.1, max_iter=2000)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print('The accuracy of SVM is ', svm_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 4. Forward Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Note: When tuning all the following neural networks, I tried to tune the activation functions,\n",
    "#### number of batches, number of epochs using val set, learning rate, and GD methods.\n",
    "#### I found the performance of the networks is highly depend on the sampled data, even though I sampled data\n",
    "#### in the same way, they can give very different results. So I mainly focused on comparison of the networks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (a) Using average word2vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "outputs": [],
   "source": [
    "# build the Feedforward Neural Network structure\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # number of nodes in each layer\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1) # use softmax to generate output\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "outputs": [],
   "source": [
    "# use cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam gradient descent and learning rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "outputs": [],
   "source": [
    "# prepare data, DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "new_train = train.copy()\n",
    "new_test = test.copy()\n",
    "\n",
    "# modify the label to match the NN's indexing\n",
    "new_train['star_rating'] = new_train['star_rating'] - 1\n",
    "new_test['star_rating'] = new_test['star_rating'] - 1\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.input = dataframe['input'].values\n",
    "        self.output = dataframe['star_rating'].values\n",
    "\n",
    "        self.length = len(self.output)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.output[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "training = MyData(new_train)\n",
    "testing = MyData(new_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=training,batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=testing, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.939405\n",
      "Epoch: 2 \tTraining Loss: 0.889882\n",
      "Epoch: 3 \tTraining Loss: 0.883836\n",
      "Epoch: 4 \tTraining Loss: 0.879370\n",
      "Epoch: 5 \tTraining Loss: 0.875800\n",
      "Epoch: 6 \tTraining Loss: 0.874333\n",
      "Epoch: 7 \tTraining Loss: 0.871841\n",
      "Epoch: 8 \tTraining Loss: 0.868916\n",
      "Epoch: 9 \tTraining Loss: 0.867375\n",
      "Epoch: 10 \tTraining Loss: 0.866241\n",
      "Epoch: 11 \tTraining Loss: 0.863411\n",
      "Epoch: 12 \tTraining Loss: 0.861644\n",
      "Epoch: 13 \tTraining Loss: 0.859459\n",
      "Epoch: 14 \tTraining Loss: 0.858097\n",
      "Epoch: 15 \tTraining Loss: 0.855882\n",
      "Epoch: 16 \tTraining Loss: 0.853982\n",
      "Epoch: 17 \tTraining Loss: 0.852472\n",
      "Epoch: 18 \tTraining Loss: 0.850258\n",
      "Epoch: 19 \tTraining Loss: 0.848851\n",
      "Epoch: 20 \tTraining Loss: 0.845609\n",
      "Epoch: 21 \tTraining Loss: 0.844023\n",
      "Epoch: 22 \tTraining Loss: 0.843476\n",
      "Epoch: 23 \tTraining Loss: 0.840219\n",
      "Epoch: 24 \tTraining Loss: 0.838527\n",
      "Epoch: 25 \tTraining Loss: 0.836811\n",
      "Epoch: 26 \tTraining Loss: 0.834085\n",
      "Epoch: 27 \tTraining Loss: 0.832646\n",
      "Epoch: 28 \tTraining Loss: 0.830992\n",
      "Epoch: 29 \tTraining Loss: 0.828700\n",
      "Epoch: 30 \tTraining Loss: 0.826890\n",
      "Epoch: 31 \tTraining Loss: 0.825534\n",
      "Epoch: 32 \tTraining Loss: 0.823809\n",
      "Epoch: 33 \tTraining Loss: 0.821789\n",
      "Epoch: 34 \tTraining Loss: 0.819789\n",
      "Epoch: 35 \tTraining Loss: 0.817947\n",
      "Epoch: 36 \tTraining Loss: 0.817075\n",
      "Epoch: 37 \tTraining Loss: 0.814630\n",
      "Epoch: 38 \tTraining Loss: 0.813607\n",
      "Epoch: 39 \tTraining Loss: 0.811126\n",
      "Epoch: 40 \tTraining Loss: 0.809463\n",
      "Epoch: 41 \tTraining Loss: 0.808294\n",
      "Epoch: 42 \tTraining Loss: 0.806467\n",
      "Epoch: 43 \tTraining Loss: 0.804800\n",
      "Epoch: 44 \tTraining Loss: 0.803803\n",
      "Epoch: 45 \tTraining Loss: 0.801736\n",
      "Epoch: 46 \tTraining Loss: 0.800293\n",
      "Epoch: 47 \tTraining Loss: 0.800136\n",
      "Epoch: 48 \tTraining Loss: 0.796902\n",
      "Epoch: 49 \tTraining Loss: 0.795966\n",
      "Epoch: 50 \tTraining Loss: 0.794391\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "n_epochs = 50 # number of epochs to train the model\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    # compute loss\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss,))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of MLP using average word2vec is  0.657488527325824\n"
     ]
    }
   ],
   "source": [
    "# predict using test set and calculate accuracy\n",
    "count = 0\n",
    "for data, target in test_loader:\n",
    "    data = data.to(torch.float32)\n",
    "    output = model(data)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    if pred == target:\n",
    "        count += 1\n",
    "print('The accuracy of MLP using average word2vec is ',count/len(test_loader.dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (b) Using concat feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "outputs": [],
   "source": [
    "# compute the concatenate word2vec feature of each review as input\n",
    "def getConcatFeature(text):\n",
    "    words = text.split()\n",
    "    vector = []\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            vector.extend(wv[word].tolist())\n",
    "            count += 1\n",
    "        if count == 10:\n",
    "            break\n",
    "    if count != 10:\n",
    "        for i in range(10 - count):\n",
    "            newVec = [0 for i in range(300)]\n",
    "            vector.extend(newVec)\n",
    "\n",
    "    return np.array(vector)\n",
    "\n",
    "train_4b = train_data.copy()\n",
    "test_4b = test_data.copy()\n",
    "train_4b['input'] = train_4b['review'].apply(getConcatFeature)\n",
    "test_4b['input'] = test_4b['review'].apply(getConcatFeature)\n",
    "train_4b = train_4b.drop(columns='review')\n",
    "test_4b = test_4b.drop(columns='review')\n",
    "train_4b = train_4b.dropna(axis = 0, how = 'any')\n",
    "test_4b = test_4b.dropna(axis = 0, how = 'any')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "outputs": [],
   "source": [
    "# build the Feedforward Neural Network structure\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "outputs": [],
   "source": [
    "# use cross-entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam gradient descent and learning rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "outputs": [],
   "source": [
    "# prepare data and DataLoader\n",
    "new_train = train_4b.copy()\n",
    "new_test = test_4b.copy()\n",
    "new_train['star_rating'] = new_train['star_rating'] - 1\n",
    "new_test['star_rating'] = new_test['star_rating'] - 1\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.input = dataframe['input'].values\n",
    "        self.output = dataframe['star_rating'].values\n",
    "\n",
    "        self.length = len(self.output)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.output[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "training = MyData(new_train)\n",
    "testing = MyData(new_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=training, shuffle=True)\n",
    "test_loader = DataLoader(dataset=testing, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.097377\n",
      "Epoch: 2 \tTraining Loss: 1.066973\n",
      "Epoch: 3 \tTraining Loss: 0.993504\n",
      "Epoch: 4 \tTraining Loss: 0.961152\n",
      "Epoch: 5 \tTraining Loss: 0.940906\n",
      "Epoch: 6 \tTraining Loss: 0.928071\n",
      "Epoch: 7 \tTraining Loss: 0.919188\n",
      "Epoch: 8 \tTraining Loss: 0.911365\n",
      "Epoch: 9 \tTraining Loss: 0.905841\n",
      "Epoch: 10 \tTraining Loss: 0.900504\n",
      "Epoch: 11 \tTraining Loss: 0.895248\n",
      "Epoch: 12 \tTraining Loss: 0.891395\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 12 # number of epochs to train the model\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    '''\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        output = model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        if pred == target:\n",
    "            count += 1\n",
    "    print(count/len(test_loader.dataset))\n",
    "    '''\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        train_loss,\n",
    "        #valid_loss\n",
    "        ))\n",
    "\n",
    "    # save model if validation loss has decreased"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of MLP using concat word2vec is 0.5648333333333333\n"
     ]
    }
   ],
   "source": [
    "# predict on test set and calculate accuracy\n",
    "count = 0\n",
    "for data, target in test_loader:\n",
    "    data = data.to(torch.float32)\n",
    "    output = model(data)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    if pred == target:\n",
    "        count += 1\n",
    "print('The accuracy of MLP using concat word2vec is',count/len(test_loader.dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 5. Recurrent Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (a) RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "outputs": [],
   "source": [
    "# compute sequential word2vec feature of each review as input\n",
    "def getRNNFeature(text):\n",
    "    words = text.split()\n",
    "    vector = []\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in wv:\n",
    "            vector.append(wv[word].tolist())\n",
    "            count += 1\n",
    "        if count == 20:\n",
    "            break\n",
    "    if count != 20:\n",
    "        for i in range(20 - count):\n",
    "            newVec = [0 for i in range(300)]\n",
    "            vector.append(newVec)\n",
    "\n",
    "    return np.array(vector)\n",
    "\n",
    "train_5a = train_data.copy()\n",
    "test_5a = test_data.copy()\n",
    "train_5a['input'] = train_5a['review'].apply(getRNNFeature)\n",
    "test_5a['input'] = test_5a['review'].apply(getRNNFeature)\n",
    "train_5a = train_5a.drop(columns='review')\n",
    "test_5a = test_5a.drop(columns='review')\n",
    "train_5a = train_5a.dropna(axis = 0, how = 'any')\n",
    "test_5a = test_5a.dropna(axis = 0, how = 'any')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "outputs": [],
   "source": [
    "# prepare data and DataLoader\n",
    "rnn_train = train_5a.copy()\n",
    "rnn_test = test_5a.copy()\n",
    "rnn_train['star_rating'] = rnn_train['star_rating'] - 1\n",
    "rnn_test['star_rating'] = rnn_test['star_rating'] - 1\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.input = dataframe['input'].values\n",
    "        self.output = dataframe['star_rating'].values\n",
    "\n",
    "        self.length = len(self.output)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.output[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "training = MyData(rnn_train)\n",
    "testing = MyData(rnn_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=training,batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=testing, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "outputs": [],
   "source": [
    "# build the RNN structure\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_num):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=self.layer_num, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_num, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        output, _ = self.rnn(x, h0.detach())\n",
    "        output = output[:,-1,:]\n",
    "        output = F.relu(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_dim)\n",
    "\n",
    "\n",
    "rnn_model = RNN(300, 20, 3, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "outputs": [],
   "source": [
    "# use CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam GD and learning rate = 0.0005\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0005)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.031782\n",
      "Epoch: 2 \tTraining Loss: 0.878597\n",
      "Epoch: 3 \tTraining Loss: 0.837002\n",
      "Epoch: 4 \tTraining Loss: 0.812297\n",
      "Epoch: 5 \tTraining Loss: 0.797108\n",
      "Epoch: 6 \tTraining Loss: 0.783459\n",
      "Epoch: 7 \tTraining Loss: 0.774925\n",
      "Epoch: 8 \tTraining Loss: 0.760690\n",
      "Epoch: 9 \tTraining Loss: 0.751393\n",
      "Epoch: 10 \tTraining Loss: 0.742999\n",
      "Epoch: 11 \tTraining Loss: 0.736459\n",
      "Epoch: 12 \tTraining Loss: 0.730472\n",
      "Epoch: 13 \tTraining Loss: 0.726213\n",
      "Epoch: 14 \tTraining Loss: 0.720817\n",
      "Epoch: 15 \tTraining Loss: 0.717068\n",
      "Epoch: 16 \tTraining Loss: 0.713203\n",
      "Epoch: 17 \tTraining Loss: 0.708905\n",
      "Epoch: 18 \tTraining Loss: 0.705315\n",
      "Epoch: 19 \tTraining Loss: 0.702833\n",
      "Epoch: 20 \tTraining Loss: 0.698327\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    rnn_model.train()\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = rnn_model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    '''\n",
    "    count = 0\n",
    "    rnn_model.eval()\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        output = rnn_model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        if pred == target:\n",
    "            count += 1\n",
    "    print('accuracy on test is: ', count/len(test_loader.dataset))\n",
    "    '''\n",
    "\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of RNN on test set is  0.6511666666666667\n"
     ]
    }
   ],
   "source": [
    "# predict using the test set and calculate accuracy\n",
    "count = 0\n",
    "rnn_model.eval()\n",
    "for data, target in test_loader:\n",
    "    data = data.to(torch.float32)\n",
    "    output = rnn_model(data)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    if pred == target:\n",
    "        count += 1\n",
    "print('The accuracy of RNN on test set is ', count/len(test_loader.dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (b) GRU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "outputs": [],
   "source": [
    "# build the RNN with GRU cells\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_num):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=self.layer_num, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_num, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        output, _ = self.rnn(x, h0.detach())\n",
    "        output = output[:,-1,:]\n",
    "        output = F.relu(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_dim)\n",
    "\n",
    "\n",
    "gru_model = GRU(300, 20, 3, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "outputs": [],
   "source": [
    "# use CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam GD and learning rate = 0.0005\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.0005)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.957697\n",
      "Epoch: 2 \tTraining Loss: 0.846718\n",
      "Epoch: 3 \tTraining Loss: 0.796572\n",
      "Epoch: 4 \tTraining Loss: 0.770886\n",
      "Epoch: 5 \tTraining Loss: 0.753284\n",
      "Epoch: 6 \tTraining Loss: 0.741363\n",
      "Epoch: 7 \tTraining Loss: 0.729519\n",
      "Epoch: 8 \tTraining Loss: 0.721023\n",
      "Epoch: 9 \tTraining Loss: 0.711605\n",
      "Epoch: 10 \tTraining Loss: 0.704843\n",
      "Epoch: 11 \tTraining Loss: 0.699004\n",
      "Epoch: 12 \tTraining Loss: 0.691164\n",
      "Epoch: 13 \tTraining Loss: 0.684915\n",
      "Epoch: 14 \tTraining Loss: 0.679431\n",
      "Epoch: 15 \tTraining Loss: 0.674279\n",
      "Epoch: 16 \tTraining Loss: 0.668736\n",
      "Epoch: 17 \tTraining Loss: 0.664794\n",
      "Epoch: 18 \tTraining Loss: 0.658272\n",
      "Epoch: 19 \tTraining Loss: 0.653814\n",
      "Epoch: 20 \tTraining Loss: 0.650249\n",
      "Epoch: 21 \tTraining Loss: 0.646058\n",
      "Epoch: 22 \tTraining Loss: 0.642572\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 22\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    gru_model.train()\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = gru_model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    '''\n",
    "    count = 0\n",
    "    gru_model.eval()\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        output = gru_model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        if pred == target:\n",
    "            count += 1\n",
    "    print('accuracy on test is: ', count/len(test_loader.dataset))\n",
    "    '''\n",
    "\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of GRU on test set is  0.6669166666666667\n"
     ]
    }
   ],
   "source": [
    "# predict using the test set and calculate accuracy\n",
    "count = 0\n",
    "gru_model.eval()\n",
    "for data, target in test_loader:\n",
    "    data = data.to(torch.float32)\n",
    "    output = gru_model(data)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    if pred == target:\n",
    "        count += 1\n",
    "print('The accuracy of GRU on test set is ',count/len(test_loader.dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (c) LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "outputs": [],
   "source": [
    "# build the RNN with LSTM cells\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_num):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=self.layer_num, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #h0 = torch.zeros(self.layer_num, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        h0 = torch.zeros(self.layer_num, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_num, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        output, _ = self.rnn(x, (h0.detach(),c0.detach()))\n",
    "        output = output[:,-1,:]\n",
    "        output = F.relu(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_dim)\n",
    "\n",
    "\n",
    "lstm_model = LSTM(300, 20, 3, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "outputs": [],
   "source": [
    "# use CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam GD and learning rate = 0.0005\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.0005)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.965559\n",
      "Epoch: 2 \tTraining Loss: 0.856667\n",
      "Epoch: 3 \tTraining Loss: 0.814796\n",
      "Epoch: 4 \tTraining Loss: 0.789789\n",
      "Epoch: 5 \tTraining Loss: 0.771713\n",
      "Epoch: 6 \tTraining Loss: 0.756592\n",
      "Epoch: 7 \tTraining Loss: 0.743536\n",
      "Epoch: 8 \tTraining Loss: 0.731910\n",
      "Epoch: 9 \tTraining Loss: 0.723249\n",
      "Epoch: 10 \tTraining Loss: 0.713810\n",
      "Epoch: 11 \tTraining Loss: 0.705455\n",
      "Epoch: 12 \tTraining Loss: 0.697131\n",
      "Epoch: 13 \tTraining Loss: 0.688954\n",
      "Epoch: 14 \tTraining Loss: 0.682845\n",
      "Epoch: 15 \tTraining Loss: 0.675805\n",
      "Epoch: 16 \tTraining Loss: 0.669001\n",
      "Epoch: 17 \tTraining Loss: 0.664491\n",
      "Epoch: 18 \tTraining Loss: 0.658675\n",
      "Epoch: 19 \tTraining Loss: 0.652997\n",
      "Epoch: 20 \tTraining Loss: 0.649106\n",
      "Epoch: 21 \tTraining Loss: 0.644366\n",
      "Epoch: 22 \tTraining Loss: 0.638345\n",
      "Epoch: 23 \tTraining Loss: 0.634089\n",
      "Epoch: 24 \tTraining Loss: 0.628480\n",
      "Epoch: 25 \tTraining Loss: 0.626207\n",
      "Epoch: 26 \tTraining Loss: 0.621550\n",
      "Epoch: 27 \tTraining Loss: 0.617512\n",
      "Epoch: 28 \tTraining Loss: 0.612027\n",
      "Epoch: 29 \tTraining Loss: 0.608999\n",
      "Epoch: 30 \tTraining Loss: 0.604128\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    lstm_model.train()\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = lstm_model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    '''\n",
    "    count = 0\n",
    "    lstm_model.eval()\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        output = lstm_model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        if pred == target:\n",
    "            count += 1\n",
    "    print('accuracy on test is: ', count/len(test_loader.dataset))\n",
    "    '''\n",
    "\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of LSTM on test set is  0.66125\n"
     ]
    }
   ],
   "source": [
    "# predict using the test set and calculate accuracy\n",
    "count = 0\n",
    "lstm_model.eval()\n",
    "for data, target in test_loader:\n",
    "    data = data.to(torch.float32)\n",
    "    output = lstm_model(data)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    if pred == target:\n",
    "        count += 1\n",
    "print('The accuracy of LSTM on test set is ', count/len(test_loader.dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
